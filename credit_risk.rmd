---
title: "Credit Risk"
author: "Filip Mordarski & Mateusz Wasielewski"
date: "5 12 2020"
output: 
  pdf_document:
header-includes: \usepackage[polish]{babel}
toc: yes
toc_depth: 2
number_sections: yes
---

```{r, include=FALSE}
library(generator)
library(dplyr)
library(rjson)
library(ggplot2)
library(moments)
library(knitr)
library(caret)
library(ROCR)
library(tictoc)
knitr::opts_chunk$set(fig.width=6, fig.height=3.5, fig.align = "center", warning = F) 
# paths
#################################################

base_path <- getwd()
output_path <- paste0(base_path, "/data")

# libnames
#################################################


clients <- read.csv2(paste0(output_path, "/clients.csv"))
loans <- read.csv2(paste0(output_path, "/loans.csv"))

data <- left_join(clients, loans, by='ID')
data <- data[-8]
```

# Wstęp

Poniższy raport będzie zawierał analizę modelu ryzyka kredytowego.

\newpage
# Tworzenie zmiennych objaśniających

W pierwszej kolejności zostały utworzone zmienne objaśniające potrzebne do modelu PD. Pierwszą zmienną, która została wygenerowana na podstawie utworzonych wcześniej zmiennych jest wiek. Został wyliczony okres trwania umowy w latach na podstawie różnicy między obecną datą a wartością w zmiennej *agreement_start*. Następnie został wygenerowany wektor wartości z rozkładu gamma z parametrem kształtu równym 3 oraz parametrem skali równym 2. Wiek został wyznaczony dodając do siebie: czas trwania umowy, liczbę 18 (wiek kiedy człowiek może podpisać wiążącą umowę kredytową) oraz wylosowaną wartość z rozkładu gamma, oznaczającą różnicę w latach pomiędzy datą podpisania umowy a osiągnięciem pełnoletności. Poniższy wykres przedstawia gęstość tej utworzonej zmiennej w naszym zbiorze.

```{r, echo=FALSE, include=T}
# tworzenie zmiennych
set.seed(1892)
# sprawdzamy czas trwania umow
agreement_time <- abs(as.numeric((Sys.Date()-as.Date(data$agreement_start))/365))
data$age <- round(agreement_time+18+rgamma(5000,shape=5,scale=1),0)
plot(density(data$age), main='Wykres gęstości wieku', ylab='Gęstość')
```

Następnie została wygenerowana zmienna, która określa czy dany pracownik jest zatrudniony, czy też nie. Prawdopodobieństwo bezrobocia zostało ustalone na poziomie 6.7 %. Wartość ta odzwierciedla średnią stopę bezrobocia w 2019 roku w Stanach Zjednoczonych. Poniższy wykres przedstawia histogram tej zmiennej w zbiorze.


```{r, echo=FALSE, include=T}
data$employed <- sample.int(2, 5000, replace = T, prob = c(0.067, 0.933))-1

ggplot(data, aes(x = factor(employed))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  scale_x_discrete(breaks=c(0,1), labels=c("Nie", "Tak"))+
  labs(title='Histogram zmiennej, określająca czy klient jest zatrudniony',x='', y="Częstość")+
  theme_minimal()
```

Na podstawie zmiennej, określającej czy dana osoba jest zatrudniona, wygenerowano zmienną czy dana osoba jest zatrudniona na pełny etat. Prawdopodobieństwo tego wynosi 80 %. Poniższy wykres przedstawia histogram tej zmiennej.

```{r, echo=F}
data$full_time <- 0
data[data$employed==1,"full_time"] <- sample.int(2, nrow(data[data$employed==1,]), replace = T, prob = c(0.2, 0.8))-1
ggplot(data, aes(x = factor(full_time))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  scale_x_discrete(breaks=c(0,1), labels=c("Nie", "Tak"))+
  labs(title='Histogram zmiennej, określająca czy klient pracuje na pełen etat',x='', y="Częstość")+
  theme_minimal()
```

Zmienną, która z pewnością może okazać się istotna w tworzeniu modelu PD jest dochód roczny danej osoby. Wartości te zostały wylosowane z rozkładu normalnego. Średnia dla osób zatrudnionych na pełen etat została ustalona na poziomie 48000 USD z odchyleniem standardowym na poziomie 15000 USD. Dla osób niezatrudnionych na pełen etat wartość średnia została ustalona na poziomie 20000 USD, natomiast odchylenie 10000 USD. Poniżej zaprezentowano wykres gęstości tej zmiennej.

```{r, echo=F}
options(scipen=10000)
data$annual_income <- abs(round(rnorm(5000,48000,40000),0))
data[data$full_time==0,"annual_income"] <- abs(round(rnorm(nrow(data[data$full_time==0,]),20000,10000),0))
plot(density(data$annual_income), main='Wykres gęstości zarobków', ylab='Gęstość')
```

Kolejno, została wygenerowana zmienna, mówiąca o tym czy dana osoba jest singlem, czy żyje w związku z inną osobą. Prawdopodobieństwo, że ktoś jest singlem w Stanach Zjednoczoncyh wynosi 30 %. Przedstawiono histogram tej zmiennej w zbiorze.

```{r, echo=F}
data$single <- sample.int(2,5000,replace = T, prob = c(0.7, 0.3))-1
ggplot(data, aes(x = factor(single))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  scale_x_discrete(breaks=c(0,1), labels=c("Nie", "Tak"))+
  labs(title='Histogram zmiennej, określająca czy klient jest singlem',x='', y="Częstość")+
  theme_minimal()
```

Następnie, została wygenerowana liczba posiadanych dzieci przez daną osobą. Zmienna ta została wygenerowana na podstawie zmiennej, określającej czy dana osoba jest singlem czy nie jest. Poniżej przedstawiono histogram tej wygenerowanej zmiennej.

```{r, echo=F}
data$kids <- round(rgamma(5000,shape=2,scale=0.8),0)
data[data$single==0, 'kids'] = round(rgamma(nrow(data[data$single==0,]),shape=1,scale=0.3),0)
data$single <- sample.int(2,5000,replace = T, prob = c(0.502, 0.498))-1
ggplot(data, aes(x = factor(kids))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  labs(title='Histogram liczby posiadanych dzieci',x='', y="Częstość")+
  theme_minimal()
```

Kolejno, została wygenerowana zmienna, mówiąca o liczbie posiadanych samochodów przez klienta. Dla osób nieposiadających dzieci lub mających jedno dziecko, liczba ta została wylosowana z następującego zakresu: [0, 1, 2] z prawdopodobieństwami równymi kolejno: [40%, 50%, 10%]. Dla klientów mających więcej niż jedno dziecko, liczba samochodów zależy od liczby posiadanych dzieci i jest obliczona za pomocą następującej formuły: liczba_dzieci - [wartość z losowania liczb [1,2] z 50% prawdopodobieństwami] + 1. Poniżej przedstawiono histogram liczby posiadanych samochodów przez klientów.

```{r, echo=F}


data$car <- sample.int(3,5000,replace = T, prob=c(0.4, 0.5, 0.1))-1
data[data$kids>1,"car"] <- data$kids[data$kids>1] - sample.int(2,nrow(data[data$kids>1,]),replace = T)+1
ggplot(data, aes(x = factor(car))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  labs(title='Histogram liczby posiadanych samochodów',x='', y="Częstość")+
  theme_minimal()
```

Następnie, została wygenerowana zmienna porządkowa, określająca sektor gospodarki, w którym pracują klienci. Prawdopodobieństwo wystąpienia następujących sektorów [rolnictwo, przemysł, usługi] wśród zatrudnionych wynosi kolejno: [20%, 30%, 50%]. Poniżej przedstawiono histogram sektorów gospodarki. 

```{r, echo=F}
# data$sector <- "Brak zatrudnienia"
data[data$employed==1, 'sector'] <- sample.int(3,nrow(data[data$employed==1,]),replace = T, prob = c(0.2,0.3,0.5))
ggplot(data%>%filter(employed==1), aes(x = factor(sector))) +
  geom_bar(fill = "#0073C2FF", stat = "count")+
  labs(title='Histogram sektorów gospodarki',x='', y="Częstość")+
  theme_minimal() +
  scale_x_discrete(labels=c('Rolnictwo', 'Przemysł', 'Usługi'))
```
# Badanie rozkładów zmiennych
W celu sprawdzenia czy zmienne objaśniajce mają wiele wartości odstających, zbadano kurtozę każdej zmiennej ciągłej. Przyjęto, że dla tej miary spłaszczenia tolerowaną wartością będzie 3. 

```{r, echo=F}
contin_vars <- data%>%
  select(value_mortgage,value_nonmortgage,age,annual_income)
y <- c()
kurtozy <- function (x)
{
  for (i in 1:ncol(x)) {
   y[i] <- kurtosis(x[i])
  }
  y
}

kable(t(kurtozy(contin_vars)),col.names = names(contin_vars), caption="Zestawienie wartości kurtozy zmiennych objaśniających")

```

Z powyższej tabeli wynika, że dla wartości kredytu hipotecznego oraz wartości kredytu bez hipoteki kurtozy wyniosły ok. 5.5, a co za tym idzie, przekraczają ustaloną wartość graniczną. Jest to spowodowane licznymi wartościami odstającymi, których wpływ na analizę trzeba zminimalizować. Niepokojąca wartość kurtozy występuje zarówno przy zmiennej roczny przychód, ponieważ wynosi powyżej 3. Istnieją różne metody radzenia sobie z tzw. "outliersami", jedna z nich zostanie przedstawiona w następnym akapicie. Dla zmiennej wiek wartość kurtozy jest mniejsza od 3, co pozwala zakładać normalność rozkładu tej zmiennej.

# Winsoryzacja zmiennych zaburzonych nietypowymi wartosciami

Podczas procesu winsoryzacji wartości odstające nie są usuwane, a jedynie podmieniane na ostatnie wartości znajdujące się w nieobciętym obszarze, dzięki czemu nie tracimy liczby obserwacji. W tym przypadku odcięte zostały skrajne obszary 5-procentowe. Poniższa tabela prezentuje wartości kurtozy po winsoryzacji tych trzech zmiennych ciągłych, których wspomniana miara spłaszczenia była większa od 3.

```{r, echo=F}
winsor1 <- function (x, fraction=.05)
{
  if(length(fraction) != 1 || fraction < 0 ||
     fraction > 0.5) {
    stop("bad value for 'fraction'")
  }
  lim <- quantile(x, probs=c(fraction, 1-fraction))
  x[ x < lim[1] ] <- lim[1]
  x[ x > lim[2] ] <- lim[2]
  x
}

data$value_mortgage_win <- winsor1(data$value_mortgage)
data$value_nonmortgage_win <- winsor1(data$value_nonmortgage)
data$annual_income_win <- winsor1(data$annual_income)
conti_win <- data%>%
  select(value_mortgage_win,value_nonmortgage_win, annual_income_win)

kable(t(kurtozy(conti_win)),col.names = names(conti_win), caption="Wartości kurtozy zmiennych po winsoryzacji")

```

Można zauważyć znaczną poprawę rozkładów tych dwóch zmiennych, co pozytywnie wpłynie na dalszą analizę.

# Szacowanie parametrów modelu

Pierwszym krokiem podczas szacowaniu modelu było podzielenie zbioru na dwa podzbiory: treningowy (3504 obserwacje) i testowy (1496 obserwacje).
```{r, echo=F}
data$employed <- factor(data$employed)
data$full_time <- factor(data$full_time)
data$single <- factor(data$single)
data[is.na(data$sector), "sector"] <- 0
data$sector <- factor(data$sector)
test_prop <- 0.3
test.set.index <- (runif(nrow(data)) < test_prop)
test <- data[test.set.index, ]
train <- data[!test.set.index, ]

```
Ze względu na fakt, że zmienna objaśniana jest binarna, zdecydowano się na oszacowanie modelu logitowego. 
```{r, echo=F}
model <- glm(default ~ value_mortgage_win+value_nonmortgage_win+age+employed+full_time+annual_income_win+single+kids+car+sector, family = binomial(link = 'logit'), data = train)
summary(model)
```
Niestety żaden z regresorów nie ma istotnego wpływu na zmienną objaśnianą, co może być spowodowane losowymi wartościami zmiennych. W przypadku zmiennej sector=3 można zaobserwować wartości NA dla współczynników równania. Powodem tego jest fakt, że zmienna sector jest zmienną porządkową i przyjmuje 4 wartości: 0,1,2,3. Model estymując współczynniki zmiennych porządkowych najpierw rozbija każdą wartość na zmienną binarną, następnie pomija jedną z nich w modelu - w tym wypadku pominął sector=0. Istotna w tym wyjaśnieniu jest pewna zależność występująca w naszym zbiorze danych: zmienna sector przyjmuje wartość 0 wtedy i tylko wtedy gdy zmienna employed, opisująca czy dany pracownik jest zatrudniony, również przyjmuje wartość 0. Z tego powodu model nie tylko pominął zmienną sector0, ale również zagregował ją w zmiennej employed=0. W wyniku tego współczynniki zmiennej sector=3 nie mogą zostać wyestymowane, gdyż teraz toona jest wykluczoną wartością zmiennej porządkowej. 

Oszacowano również drugi model, w którym zmieniliśmy założenie co do rozkładu składnika losowego. Zamiast transformacji logistycznej zastosowano odwrotność dystrybuanty rozkłądu normalnego W wyniku tego, został oszacowany model probitowy.

```{r, echo=F}
model2 <- glm(default ~ value_mortgage_win+value_nonmortgage_win+age+employed+full_time+
               annual_income_win+single+kids+car+sector, family = binomial(link = 'probit'), data = train)
summary(model2)
```
Tutaj podobnie jak w przypadku estymacji modelu logitowego pojawiają się wartości NA przy współczynnikach zmiennej sektor=3.

# Ewaluacja modelu

Po oszacowaniu obu modelów, dokonano ich ewaluacji. Poniżej przedstawiono tabelę kontyngencji dla pierwszego modelu logitowego.

```{r, echo=F}
predict <- predict(model, type = 'response', newdata = test)
predict <- ifelse(predict<0.5, 0, 1)
confusionMatrix(data=factor(predict), reference=factor(test$default))
```

Następnie wygenerowano podobną tabelę kontyngencji dla drugiego modelu - probitowego.

```{r, echo=F}
predict <- predict(model2, type = 'response', newdata = test)
predict <- ifelse(predict<0.5, 0, 1)
confusionMatrix(data=factor(predict), reference=factor(test$default))
```
Model ten dostarcza takich samych predykcji jak model pierwszy. Z tego względu dalsza analiza będzie opierać się na oszacowaniach jednego z powyższych modeli.

Na podstawie powyższych tabel możemy stwierdzić, że nasze modele nie są efektywne w predykcji 'defaultu'. Wszystkie obserwacje zostały dopasowane do kategorii 0, czyli brak 'defaultu'. Mamy relatywnie wysoki poziom trafności modeli, ponieważ jest on na poziomie 78 %. Przyczyną takiego stanu rzeczy, jest to że 78% obserwacji ze zbioru testowego nie miało 'defaultu'. Ewaluacja tych modeli wykazała, że nie powinniśmy wyciągać żadnych dalekoidących wniosków na podstawie ich oszacowania. Następnie została wygenerowana krzywa ROC.

```{r, echo=F}
ROCRpred_t <- prediction(predict, test$default)
ROCRperf_t <- performance(ROCRpred_t, 'tpr','fpr')
plot(ROCRperf_t, main='Krzywa ROC')
```

Powyższa krzywa obrazuje zdolność predykcyjną modelu dla różnych progów odcięcia. Wygląd powyższej krzywej pokrywa się z powyższą oceną modelów za pomocą tabel kontyngencji. Oszacowane modele są tak efektywne jak klasyfikator losowy. Z uwagi na to, że modele nie przewidują dla żadnego klienta wartości defaultu równego 1, czułość naszego modelu wynosi 100%, natomiast swoistość 0%. Z tego względu pole pod krzywą ROC wynosi 0.5, co potwierdza poniższy wydruk z R.

```{r, echo=T, include=T}
auc_t <- performance(ROCRpred_t, measure = "auc")
auc_t <- auc_t@y.values[[1]]
auc_t
```

```{r, echo=T, include=T}
N <- length(data$default)  # how long will be our new sample
M <- 1000 # how many times we will generate new sample
auc_vector <- c(1:M)  # starting values of AUC vector
alpha <- 0.1  

data$predict_boots <- predict(model, type = 'response', newdata = data)
data$predict_boots <- ifelse(data$predict_boots<0.5, 0, 1)

tic()

for (i in 1:M){
  
    # 1 step - drawing rows from the sample with replacement
    resample <- sample_n(data, N, replace = TRUE)

    # 2 step - compute AUC for resample
    # ROCR Curve
    ROCRpred <- prediction(resample$predict_boots, resample$default)
    ROCRperf <- performance(ROCRpred, 'tpr','fpr')
    # AUC value
    auc <- performance(ROCRpred, measure = "auc")
    auc <- auc@y.values[[1]]
    auc_vector[i] <- auc_t  # here are stored AUC values for each subsample
    
}

toc()
lower <- quantile(auc_vector, alpha / 2)

upper <- quantile(auc_vector, 1 - (alpha / 2) )

paste0('AUC = ', auc_t, ' and ', (1 - alpha) * 100,
        '% bootstrapped confidence interval for AUC is between ', lower, ' and ', upper )

```